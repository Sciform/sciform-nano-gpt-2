{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logits\n",
    "\n",
    "Logits refer to the raw, unnormalized scores output by the GPT model's final layer before applying any kind of normalization such as softmax. These logits represent the model's confidence in each possible token in the vocabulary being the next token in the sequence.\n",
    "\n",
    "Have a look at an simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode input text\n",
    "input_text = \"I am female. Hello, my name is\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Get model outputs\n",
    "outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit shape = torch.Size([1, 9, 50257])\n",
      "last token logits = tensor([[-63.0282, -64.0266, -67.8727,  ..., -73.1530, -72.5244, -64.9166]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "softmax probs for last token logits = tensor([[1.8694e-04, 6.8882e-05, 1.4715e-06,  ..., 7.4914e-09, 1.4046e-08,\n",
      "         2.8287e-05]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get logits\n",
    "logits = outputs.logits\n",
    "\n",
    "# Logits shape: (batch_size, sequence_length, vocab_size)\n",
    "print(\"logit shape =\", logits.shape)\n",
    "\n",
    "# Get logits for the last token in the input sequence.\n",
    "# The last token in the input sequence is used to predict the first token of \n",
    "# the output sequence\n",
    "last_token_logits = logits[:, -1, :]\n",
    "print(\"last token logits =\", last_token_logits)\n",
    "\n",
    "# Convert logits to probabilities using softmax\n",
    "probs = torch.softmax(last_token_logits, dim=-1)\n",
    "print(\"softmax probs for last token logits =\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next token:  Sarah\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Get the predicted token\n",
    "predicted_token_id = torch.argmax(probs, dim=-1).item()\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "print(f\"Predicted next token: {predicted_token}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
